{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "sys.path.append('../src/')\n",
    "from scraper.scraper import LiveTrailScraper\n",
    "from results.results import Results\n",
    "from database.create_db import Database\n",
    "from database.database import Event, Race, Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database already exists.\n"
     ]
    }
   ],
   "source": [
    "db: Database = Database.create_database(path='../data/parsed_data.db')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = LiveTrailScraper()\n",
    "events = scraper.get_events()\n",
    "years = scraper.get_events_years()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of events and years\n",
    "events = dict(sorted(scraper.get_events().items(), key=lambda item: item[1]))\n",
    "# Remove years and strip\n",
    "events = {key: ' '.join(word for word in value.split() if not word.isdigit() or len(word) != 4).strip() for key, value in events.items()}\n",
    "events = {key: re.sub(r'^\\d{4}|\\d{4}$', '', value).strip() for key, value in events.items()}\n",
    "# Remove French ordinals\n",
    "events = {key: re.sub(r'(\\d{1,2}(?:e|Ã¨me))', '', value).strip() for key, value in events.items()}\n",
    "# Remove HTML tags\n",
    "events = {key: re.sub(r'<[^<]+?>', '', value).strip() for key, value in events.items()}\n",
    "# Sort alphabetically\n",
    "events = dict(sorted(events.items(), key=lambda item: item[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code, name in events.items():\n",
    "    if code in years:\n",
    "        for year in years[code]: \n",
    "            event = Event(event_code=code,\n",
    "                        event_name=name,\n",
    "                        year=year,\n",
    "                        country=None,\n",
    "                        db=db)\n",
    "            event.save_to_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to resume from a partial download\n",
    "def parse_done_txt_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        parsed_data = {}\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 2:\n",
    "                race_name = parts[0]\n",
    "                years = [year for year in parts[1:]]\n",
    "                parsed_data.setdefault(race_name, []).extend(years)\n",
    "    return parsed_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to skip races, create a text file `output_parsing.txt` containing one code and year per line wanting to be ignored, for example:\n",
    "\n",
    "```\n",
    "saintelyon 2018\n",
    "saintelyon 2017\n",
    "saintelyon 2016\n",
    "saintelyon 2015\n",
    "saintelyon 2014\n",
    "saintelyon 2013\n",
    "penyagolosa 2024\n",
    "penyagolosa 2023\n",
    "penyagolosa 2022\n",
    "penyagolosa 2021\n",
    "penyagolosa 2019\n",
    "# lut 2016 -> parcours.php is empty\n",
    "lut 2016\n",
    "# oxfamtrailwalkerhk 2021 -> Password protected\n",
    "oxfamtrailwalkerhk 2021\n",
    "```\n",
    "\n",
    "If you interrupt the execution, copy the output of the next cell in the file and races already parsed will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_races = parse_done_txt_file('output_parsing.txt')\n",
    "for event, name in events.items():\n",
    "    if event in years:\n",
    "        for year in years[event]:\n",
    "            if event in parsed_races:\n",
    "                if year in parsed_races[event]:\n",
    "                    continue\n",
    "            print(event, year)\n",
    "            scraper.set_events([event])\n",
    "            scraper.set_years([year])\n",
    "            cps, cpns = scraper.get_control_points()\n",
    "            event_id = Event.get_id_from_code_year(event, year, db=db)\n",
    "            if not cps:\n",
    "                continue\n",
    "            races = scraper.get_races()\n",
    "            rr = scraper.get_random_runner_bib()\n",
    "            scraper.download_data()\n",
    "            races_data = scraper.get_races_physical_details()\n",
    "            if event not in races:\n",
    "                #st.write(f'No data available for {events[event]} {year}. Please select another event.')\n",
    "                pass\n",
    "            elif year not in races[event]:\n",
    "                #st.write(f'No data available for {events[event]} {year}. Please select another event or year.')\n",
    "                pass\n",
    "            else:\n",
    "                races = races[event][year]\n",
    "                for race, name in races.items():\n",
    "                    if race not in cps:\n",
    "                        continue\n",
    "                    if race=='maxirace' and 'Orientation' in name:\n",
    "                        # 'maxirace' has two orientation races not standard\n",
    "                        continue\n",
    "                    elif name.lower()=='course des partenaires':\n",
    "                        continue\n",
    "                    scraper.set_race(race)\n",
    "                    folder_path = f'data/{event}'\n",
    "                    filepath = os.path.join(folder_path, f'{event}_{race}_{year}.csv')\n",
    "                    results_filepath = filepath if os.path.exists(os.path.join('../../',filepath)) else None\n",
    "                    race_info = scraper.get_race_info(bibN=rr[year][race]) if rr[year][race] is not None else {'date':None, 'hd':None}\n",
    "                    control_points = cps[race]\n",
    "                    race_data = races_data[race]\n",
    "                    if race_info: # some races are empty but have empty rows in data (e.g. 'templiers', 'Templi', 2019)\n",
    "                        # the .split('.')[0] is needed since few races sometime contain a dot at the end or '000' for milliseconds\n",
    "                        departure_datetime = ' '.join([race_info['date'], race_info['hd']]).split('.')[0] if race_info['date'] else None\n",
    "                    else:\n",
    "                        departure_datetime = None\n",
    "                    r = Race(race_id=race, event_id=event_id, race_name=name, distance=race_data['distance'],\n",
    "                    elevation_pos=race_data['elevation_pos'], elevation_neg=race_data['elevation_pos'], departure_datetime=departure_datetime,\n",
    "                    results_filepath=results_filepath, db=db)\n",
    "                    r.save_to_database()\n",
    "                    for cp_code, data in cps[race].items():\n",
    "                        conn = sqlite3.connect(db.path, timeout=3600)\n",
    "                        with conn:\n",
    "                            cursor = conn.cursor()\n",
    "                            try:\n",
    "                                cursor.execute('''\n",
    "                                            INSERT INTO control_points\n",
    "                                                        (event_id, race_id, code, name,\n",
    "                                                        distance, elevation_pos, elevation_neg)\n",
    "                                            VALUES (?, ?, ?, ?, ?, ?, ?)''',\n",
    "                                            (event_id, race, cp_code, cpns[race][cp_code], # Name not scraped\n",
    "                                                data[0], data[1], data[2],))\n",
    "                            except sqlite3.IntegrityError:\n",
    "                                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run CSV_to_DB_results.py -p ../data/parsed_data.db -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run CSV_to_DB_timing_points.py -p ../data/parsed_data.db -c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLTrail",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
